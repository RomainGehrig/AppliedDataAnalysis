{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swiss Tweet Votations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieving\n",
    "Since we want to focus on one particular event, we don't need to work on the complete dataset. We download the tweets from the cluster to be able to work on them remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from Hadoop\n",
    "\n",
    "import glob\n",
    "from os.path import basename, splitext\n",
    "import requests as rx\n",
    "from urllib.request import urlretrieve\n",
    "from time import sleep\n",
    "\n",
    "swiss_tweet_url = 'http://iccluster060.iccluster.epfl.ch:50070/webhdfs/v1/datasets/swiss-tweet'\n",
    "operation_list = '?op=LISTSTATUS'\n",
    "operation_open = '?op=OPEN'\n",
    "\n",
    "req = rx.get(swiss_tweet_url + operation_list)\n",
    "if req.status_code != 200:\n",
    "    raise Exception(\"Failed to load list of files\")\n",
    "\n",
    "remote_swiss_tweet_files = set(map(lambda f: f['pathSuffix'], req.json()['FileStatuses']['FileStatus']))\n",
    "local_swiss_tweet_files = set(map(lambda f: basename(f), glob.glob('data/harvest3r_twitter_data_*.json')))\n",
    "\n",
    "missing_swiss_tweet_files = remote_swiss_tweet_files - local_swiss_tweet_files\n",
    "missing_index, missing_count = 0, len(missing_swiss_tweet_files)\n",
    "\n",
    "if missing_count == 0:\n",
    "    print(\"Your dataset is complete, nothing to download!\")\n",
    "\n",
    "for swiss_tweet_file in missing_swiss_tweet_files:\n",
    "    missing_index += 1\n",
    "    print(\"Downloading {} ({} of {} files)\".format(swiss_tweet_file, missing_index, missing_count))\n",
    "    \n",
    "    frm = swiss_tweet_url + '/' + swiss_tweet_file + operation_open\n",
    "    to = 'data/' + swiss_tweet_file\n",
    "    \n",
    "    urlretrieve(frm, to)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Filtering\n",
    "\n",
    "As we only want tweets containing hashtags, we keep only those and save them so there are less tweets to process afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "datasets = glob.glob('data/harvest3r_twitter_data_*.json')\n",
    "for datapath in datasets:\n",
    "    i += 1\n",
    "    print(\"Working on file {} of {}: {}\".format(i, len(datasets), datapath))\n",
    "    tweets = pd.DataFrame( pd.read_json(datapath)._source.tolist() )\n",
    "    tagged_tweets = tweets.dropna(subset=['tags'])\n",
    "    tagged_tweets.tags = tagged_tweets.tags.apply(lambda ts: set([t.lower() for t in ts]))\n",
    "    print(\"Amount of tagged tweets found: {}\".format(tagged_tweets.main.size))\n",
    "    taggedpath = 'data/tagged_' + splitext(basename(datapath))[0] + '.json'\n",
    "    print(\"Tagged tweets written to: {}\".format(taggedpath))\n",
    "    tagged_tweets.to_json(taggedpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Exploration\n",
    "\n",
    "First of all, we need to discover hashtags used for the votations. We begin our search by finding all tweets tagged #chvote, a popular hashtag when votations are around the corner. We will then count every other hashtags associated – in general, the subject of the votation is also in the hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "chvote_tweets = pd.DataFrame()\n",
    "datasets = glob.glob('data/tagged_harvest3r_twitter_data_*.json')\n",
    "for datapath in datasets:\n",
    "    i += 1\n",
    "    print(\"Working on file {} of {}: {}\".format(i, len(datasets), datapath))\n",
    "    tweets = pd.read_json(datapath)\n",
    "    tweets_found = tweets.select(lambda t: 'chvote' in tweets.loc[t].tags)\n",
    "    print(\"Amount of #chvote found: {} / {}\".format(tweets_found.main.size, tweets.main.size))\n",
    "    chvote_tweets = chvote_tweets.append(tweets_found, ignore_index=True)\n",
    "    \n",
    "chvotepath = 'data/tweets-chvote.json'\n",
    "print(\"Tweets #chvote written to: \", chvotepath)\n",
    "chvote_tweets.to_json(chvotepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract tags\n",
    "chvote_tags = list()\n",
    "for tags in chvote_tweets.tags:\n",
    "    chvote_tags += [t for t in tags]\n",
    "\n",
    "# Popular tags\n",
    "chvote_tags = pd.Series(chvote_tags)\n",
    "chvote_tags.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: selecting tweets with given tags\n",
    "\n",
    "We found a list of popular tags associated with the votations (more info on these in the README): \"Abst16\", \"AVSplus\", \"LRens\", \"RBI\". For each of those, we create a separate json file containing only those, once again to ease distribution and reuse – each generated file is less than 10MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "tags = ['abst16', 'avsplus', 'lrens', 'rbi']\n",
    "tweets_by_tag = { tag: pd.DataFrame() for tag in tags }\n",
    "\n",
    "# Collect tweets by tags\n",
    "datasets = glob.glob('data/tagged_harvest3r_twitter_data_*.json')\n",
    "for datapath in datasets:\n",
    "    i += 1\n",
    "    print(\"Working on file {} of {}: {}\".format(i, len(datasets), datapath))\n",
    "    tweets = pd.read_json(datapath)\n",
    "    for tag in tweets_by_tag.keys():\n",
    "        tweets_found = tweets.select(lambda t: tag in tweets.loc[t].tags)\n",
    "        tweets_by_tag[tag] = tweets_by_tag[tag].append(tweets_found, ignore_index=True)\n",
    "        print(\"Amount of #{} found: {} / {}\".format(tag, tweets_found.main.size, tweets.main.size))\n",
    "        print(\"Current amount of #{}: {}\".format(tag, tweets_by_tag[tag].main.size))\n",
    "\n",
    "# Write tweets for future uses\n",
    "for tag, tweets_tag in tweets_by_tag.items():\n",
    "    tagpath = \"data/tweets-{}.json\".format(tag)\n",
    "    print(\"Writting {} tweets to {}\".format(tweets_tag.main.size, tagpath))\n",
    "    tweets_tag.to_json(tagpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display stats on collected tags\n",
    "tags = ['abst16', 'avsplus', 'lrens', 'rbi']\n",
    "for t in tags:\n",
    "    ts = pd.read_json('data/tweets-{}.json'.format(t))\n",
    "    ts = ts.dropna(subset=['main'])\n",
    "    print(\"Tweets for {}: {}\".format(t, ts.main.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of tweets being pretty low, we need to find more of them. The issue is that a votation is referred with different hashtags, for instance the name of the law in different languages. Looking for only one of them means we miss some part of the discussions. Looking to the other tags associated with our first selection can indicate us which other hashtags has been used. We also made some research manually on Twitter to find the other hashtags. We decided to drop #avsplus and came came up with the following list for each votation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "votations = {\n",
    "    'abst16': {'abst16', 'abstimmung', 'abstimmungen', 'atomausstieg', 'atomausstiegja', 'atomausstiegsinitiative', 'ausstiegsinitiative', 'energiewende', 'sortirdunucleaire', 'atome', 'nucleaire', 'nucléaire'},\n",
    "    'lrens': {'lrens', 'lrensnon', 'lrensoui', 'loirenseignement', 'lscpt', 'etatfouineur', 'saveprivacy', 'ndg', 'ndgnein', 'stopndg', 'ndgja', 'ndb', 'stopndb', 'src', 'bupf', 'surveillance', 'privacy'},\n",
    "    'rbi': {'rbi', 'rbioui', 'rentabásicauniversal', 'revenuuniversel', 'ubi', 'basicincome', 'grundeinkommen', 'rbi16', 'rbi2016', 'revenudebase', 'revenuuniversel', 'allocationuniverselle'}\n",
    "}\n",
    "tweets_by_votation = { votation: pd.DataFrame() for votation in votations.keys() }\n",
    "\n",
    "# Collect tweets by tags\n",
    "datasets = glob.glob('data/tagged_harvest3r_twitter_data_*.json')\n",
    "for datapath in datasets:\n",
    "    i += 1\n",
    "    print(\"Working on file {} of {}: {}\".format(i, len(datasets), datapath))\n",
    "    tweets = pd.read_json(datapath)\n",
    "    for votation, tags_set in votations.items():\n",
    "        tweets_found = tweets.select(lambda t: votation in tweets.loc[t].tags)\n",
    "        tweets_tag = tweets_tag.append(tweets_found, ignore_index=True)\n",
    "        print(\"Amount of #{} found: {} / {}\".format(votation, tweets_found.main.size, tweets.main.size))\n",
    "        print(\"Current amount of #{}: {}\".format(votation, tweets_tag.main.size))\n",
    "\n",
    "# Write tweets for future uses\n",
    "for tag, tweets_tag in tweets_by_tag.items():\n",
    "    tagpath = \"data/tweets-{}-full.json\".format(tag)\n",
    "    print(\"Writting {} tweets to {}\".format(tweets_tag.main.size, tagpath))\n",
    "    tweets_tag.to_json(tagpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "There, we look at an example a votation (LRens) to get a grip of the kind of data we have and do some basic tests/exploration around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_json('data/tweets-lrens.json')\n",
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, a time plot of the number of tweets mentionning #LRens (votation of the 25 sept 2016)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "date_parser = lambda d: datetime.strptime(d[:10], \"%Y-%m-%d\").date()\n",
    "tweets['date_found'] = tweets['date_found'].apply(date_parser)\n",
    "counts = pd.DataFrame({\"count\": tweets.groupby('date_found').size()})\n",
    "plt.figure()\n",
    "counts.plot(kind='bar', figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumb test to see the relevance of a dirt simple classifier (to classify tweets into \"will vote YES/NO\")\n",
    "\n",
    "noes = [\"non\", \"nein\", \"no\"] \n",
    "yess = ['oui', 'ya', 'yes']\n",
    "\n",
    "no_tweets = []\n",
    "yes_tweets = []\n",
    "\n",
    "for i,c in df.iterrows():\n",
    "    comparable = c['main'].lower().split()\n",
    "    if any([no in comparable for no in noes]):\n",
    "        print(\"NO:  \", c['main'], c['tags'])\n",
    "        no_tweets.append(c['main'])\n",
    "    if any([yes in comparable for yes in yess]):\n",
    "        print(\"YES: \", c['main'], c['tags'])\n",
    "        yes_tweets.append(c['main'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The exploration that made us a bit skeptical of basing our analysis on the sentiment score... \n",
    "\n",
    "First we can see that there are less than 5% of the tweets that aren't classified as \"NEUTRAL\" which is quite a low number to base our observations on (especially if there isn't much data to begin with). Then, by looking at the tweets we have, it's easy to see – as humans – that the subject is polarizing and there are subtleties and references to History that are hard to pickup by a computer. Example:\n",
    "  \"@benoitgaillard ouais, on a vu, le sujet des écoutes allemandes a été fort bien traité avant la votation sur #LRens -.-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets['sentiment'].value_counts())\n",
    "\n",
    "for idx, t in tweets[tweets['sentiment'] == 'NEUTRAL'].iterrows():\n",
    "    print(t['main'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Positive tweets set\")\n",
    "pos_train_tweets_30 = pd.read_json('data/pos_harvest3r_twitter_data_30-10_0.json')\n",
    "pos_train_tweets_31 = pd.read_json('data/pos_harvest3r_twitter_data_31-10_0.json')\n",
    "print(\"Sizes of 30 ({}) and 31 ({})\".format(pos_train_tweets_30.main.size, pos_train_tweets_31.main.size))\n",
    "print(\"Expected size: {}\".format(pos_train_tweets_30.size + pos_train_tweets_31.size))\n",
    "pos_train_tweets = pos_train_tweets_30.append(pos_train_tweets_31, ignore_index=True)\n",
    "print(\"Final size: {}\".format(pos_train_tweets.main.size))\n",
    "\n",
    "print(\"# Negative tweets set\")\n",
    "neg_train_tweets_30 = pd.read_json('data/neg_harvest3r_twitter_data_30-10_0.json')\n",
    "neg_train_tweets_31 = pd.read_json('data/neg_harvest3r_twitter_data_31-10_0.json')\n",
    "print(\"Sizes of 30 ({}) and 31 ({})\".format(neg_train_tweets_30.size, neg_train_tweets_31.size))\n",
    "print(\"Expected size: {}\".format(neg_train_tweets_30.size + neg_train_tweets_31.size))\n",
    "neg_train_tweets = neg_train_tweets_30.append(neg_train_tweets_31, ignore_index=True)\n",
    "print(\"Final size: {}\".format(neg_train_tweets.main.size))\n",
    "\n",
    "print(\"# Full tweets set\")\n",
    "train_tweets = pos_train_tweets.append(neg_train_tweets, ignore_index=True)\n",
    "print(\"Final size: {}\".format(train_tweets.main.size))\n",
    "\n",
    "print(\"# Test tweets set\")\n",
    "pos_test_tweets = pd.read_json('data/pos_harvest3r_twitter_data_01-09_0.json')\n",
    "neg_test_tweets = pd.read_json('data/neg_harvest3r_twitter_data_01-09_0.json')\n",
    "test_tweets = pos_test_tweets.append(neg_test_tweets, ignore_index=True)\n",
    "print(\"Final size: {}\".format(test_tweets.main.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_train_tweets = vectorizer.fit_transform(train_tweets.main)\n",
    "classifier = SGDClassifier(loss='modified_huber', penalty='l2', alpha=2e-06, max_iter=100, n_jobs=-1)\n",
    "model = classifier.fit(vectorized_train_tweets, train_tweets.sentiment)\n",
    "print(\"Cross validation: \", np.mean(cross_val_score(classifier, vectorized_train_tweets, train_tweets.sentiment, cv=5, n_jobs=-1)))\n",
    "\n",
    "vectorized_test_tweets = vectorizer.transform(test_tweets.main)\n",
    "test_tweets['our_sentiment'] = model.predict(vectorized_test_tweets)\n",
    "accuracy_score(test_tweets.sentiment, test_tweets.our_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "date2week = lambda y, m, d: int(dt.date(y, m, d).strftime('%W'))\n",
    "date_parser = lambda d: dt.datetime.strptime(d[:10], \"%Y-%m-%d\").date()\n",
    "\n",
    "tags = ['abst16', 'lrens', 'rbi']\n",
    "tweets_by_tags = dict()\n",
    "votation_dates = { 'abst16': date2week(2016, 11, 27), 'lrens': date2week(2016, 9, 25), 'rbi': date2week(2016, 6, 5) }\n",
    "\n",
    "for tag in tags:\n",
    "    tweets = pd.read_json('data/tweets-{}.json'.format(tag))\n",
    "    vectorized_tweets = vectorizer.transform(tweets.main)\n",
    "    tweets['date_found'] = tweets.date_found.apply(date_parser)\n",
    "    tweets['our_sentiment'] = model.predict(vectorized_tweets)\n",
    "    tweets.votation_week = votation_dates[tag]\n",
    "    tweets_by_tags[tag] = tweets\n",
    "    for (id, t) in tweets[['main', 'our_sentiment']].head(10).iterrows():\n",
    "        print(\"{}: {}\".format(t.our_sentiment, t.main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag, tweets in tweets_by_tags.items():\n",
    "    print(\"#{}: {}\".format(tag, tweets.main.size))\n",
    "    hists = {}\n",
    "    tweets['week_found'] = tweets.date_found.apply(lambda d: int(d.strftime('%W')))\n",
    "    for sentiment, data in tweets.groupby('our_sentiment'):\n",
    "        hist = np.histogram(data.week_found, bins=52, range=[1, 52])\n",
    "        hists[sentiment] = hist[0]\n",
    "        index = hist[1][1:]\n",
    "\n",
    "    pd.DataFrame(hists, columns=['POSITIVE', 'NEGATIVE']).plot.bar(figsize = (12, 6), width = .9)\n",
    "    plt.axvline(tweets.votation_week + 0.45, color='red', linewidth=1)\n",
    "    \n",
    "    plt.title('#' + tag)\n",
    "    plt.xlabel('Weeks of 2016')\n",
    "    plt.ylabel('Tweets count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
