{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading harvest3r_twitter_data_07-09_0.json (1 of 301 files)\n",
      "Downloading harvest3r_twitter_data_06-01_0.json (2 of 301 files)\n",
      "Downloading harvest3r_twitter_data_05-02_0.json (3 of 301 files)\n",
      "Downloading harvest3r_twitter_data_14-02_0.json (4 of 301 files)\n",
      "Downloading harvest3r_twitter_data_25-01_0.json (5 of 301 files)\n",
      "Downloading harvest3r_twitter_data_27-10_0.json (6 of 301 files)\n",
      "Downloading harvest3r_twitter_data_18-05_0.json (7 of 301 files)\n",
      "Downloading harvest3r_twitter_data_24-10_0.json (8 of 301 files)\n",
      "Downloading harvest3r_twitter_data_29-02_0.json (9 of 301 files)\n",
      "Downloading harvest3r_twitter_data_07-02_0.json (10 of 301 files)\n",
      "Downloading harvest3r_twitter_data_07-04_0.json (11 of 301 files)\n",
      "Downloading harvest3r_twitter_data_06-05_0.json (12 of 301 files)\n",
      "Downloading harvest3r_twitter_data_17-10_0.json (13 of 301 files)\n",
      "Downloading harvest3r_twitter_data_06-09_0.json (14 of 301 files)\n",
      "Downloading harvest3r_twitter_data_19-03_0.json (15 of 301 files)\n",
      "Downloading harvest3r_twitter_data_12-04_0.json (16 of 301 files)\n",
      "Downloading harvest3r_twitter_data_25-06_0.json (17 of 301 files)\n",
      "Downloading harvest3r_twitter_data_22-03_0.json (18 of 301 files)\n",
      "Downloading harvest3r_twitter_data_08-06_0.json (19 of 301 files)\n",
      "Downloading harvest3r_twitter_data_12-01_0.json (20 of 301 files)\n",
      "Downloading harvest3r_twitter_data_12-07_0.json (21 of 301 files)\n",
      "Downloading harvest3r_twitter_data_30-03_0.json (22 of 301 files)\n",
      "Downloading harvest3r_twitter_data_27-03_0.json (23 of 301 files)\n",
      "Downloading harvest3r_twitter_data_23-08_0.json (24 of 301 files)\n",
      "Downloading harvest3r_twitter_data_17-03_0.json (25 of 301 files)\n",
      "Downloading harvest3r_twitter_data_25-03_0.json (26 of 301 files)\n",
      "Downloading harvest3r_twitter_data_03-01_0.json (27 of 301 files)\n",
      "Downloading harvest3r_twitter_data_07-10_0.json (28 of 301 files)\n",
      "Downloading harvest3r_twitter_data_31-01_0.json (29 of 301 files)\n",
      "Downloading harvest3r_twitter_data_27-02_0.json (30 of 301 files)\n",
      "Downloading harvest3r_twitter_data_01-05_0.json (31 of 301 files)\n",
      "Downloading harvest3r_twitter_data_18-07_0.json (32 of 301 files)\n",
      "Downloading harvest3r_twitter_data_18-02_0.json (33 of 301 files)\n",
      "Downloading harvest3r_twitter_data_26-08_0.json (34 of 301 files)\n",
      "Downloading harvest3r_twitter_data_16-06_0.json (35 of 301 files)\n",
      "Downloading harvest3r_twitter_data_09-08_0.json (36 of 301 files)\n",
      "Downloading harvest3r_twitter_data_13-02_0.json (37 of 301 files)\n",
      "Downloading harvest3r_twitter_data_14-07_0.json (38 of 301 files)\n",
      "Downloading harvest3r_twitter_data_30-01_0.json (39 of 301 files)\n",
      "Downloading harvest3r_twitter_data_11-03_0.json (40 of 301 files)\n",
      "Downloading harvest3r_twitter_data_19-09_0.json (41 of 301 files)\n",
      "Downloading harvest3r_twitter_data_08-04_0.json (42 of 301 files)\n",
      "Downloading harvest3r_twitter_data_24-04_0.json (43 of 301 files)\n",
      "Downloading harvest3r_twitter_data_09-04_0.json (44 of 301 files)\n",
      "Downloading harvest3r_twitter_data_31-10_1.json (45 of 301 files)\n",
      "Downloading harvest3r_twitter_data_01-02_0.json (46 of 301 files)\n",
      "Downloading harvest3r_twitter_data_25-04_0.json (47 of 301 files)\n",
      "Downloading harvest3r_twitter_data_15-07_0.json (48 of 301 files)\n",
      "Downloading harvest3r_twitter_data_16-03_0.json (49 of 301 files)\n",
      "Downloading harvest3r_twitter_data_18-08_0.json (50 of 301 files)\n",
      "Downloading harvest3r_twitter_data_13-07_0.json (51 of 301 files)\n",
      "Downloading harvest3r_twitter_data_17-08_0.json (52 of 301 files)\n",
      "Downloading harvest3r_twitter_data_04-04_0.json (53 of 301 files)\n",
      "Downloading harvest3r_twitter_data_13-03_0.json (54 of 301 files)\n",
      "Downloading harvest3r_twitter_data_10-06_0.json (55 of 301 files)\n",
      "Downloading harvest3r_twitter_data_24-09_0.json (56 of 301 files)\n",
      "Downloading harvest3r_twitter_data_11-10_0.json (57 of 301 files)\n",
      "Downloading harvest3r_twitter_data_24-05_0.json (58 of 301 files)\n",
      "Downloading harvest3r_twitter_data_16-07_0.json (59 of 301 files)\n",
      "Downloading harvest3r_twitter_data_20-09_0.json (60 of 301 files)\n",
      "Downloading harvest3r_twitter_data_20-10_0.json (61 of 301 files)\n",
      "Downloading harvest3r_twitter_data_23-03_0.json (62 of 301 files)\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from Hadoop\n",
    "\n",
    "import glob\n",
    "import os.path\n",
    "import requests as rx\n",
    "from urllib.request import urlretrieve\n",
    "from time import sleep\n",
    "\n",
    "swiss_tweet_url = 'http://iccluster060.iccluster.epfl.ch:50070/webhdfs/v1/datasets/swiss-tweet'\n",
    "operation_list = '?op=LISTSTATUS'\n",
    "operation_open = '?op=OPEN'\n",
    "\n",
    "req = rx.get(swiss_tweet_url + operation_list)\n",
    "if req.status_code != 200:\n",
    "    raise Exception(\"Failed to load list of files\")\n",
    "\n",
    "remote_swiss_tweet_files = set(map(lambda f: f['pathSuffix'], req.json()['FileStatuses']['FileStatus']))\n",
    "local_swiss_tweet_files = set(map(lambda f: os.path.basename(f), glob.glob('data/harvest3r_twitter_data_*.json')))\n",
    "\n",
    "missing_swiss_tweet_files = remote_swiss_tweet_files - local_swiss_tweet_files\n",
    "missing_index, missing_count = 0, len(missing_swiss_tweet_files)\n",
    "\n",
    "if missing_count == 0:\n",
    "    print(\"Your dataset is complete, nothing to download!\")\n",
    "\n",
    "for swiss_tweet_file in missing_swiss_tweet_files:\n",
    "    missing_index += 1\n",
    "    print(\"Downloading {} ({} of {} files)\".format(swiss_tweet_file, missing_index, missing_count))\n",
    "    \n",
    "    frm = swiss_tweet_url + '/' + swiss_tweet_file + operation_open\n",
    "    to = 'data/' + swiss_tweet_file\n",
    "    \n",
    "    urlretrieve(frm, to)\n",
    "    sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
